{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Proyecto1B_RI_Luis_Almeida_Edison_Cabrera.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Escuela Politécnica Nacional<br>\n",
        "\n",
        "\n",
        "Facultad de Ingeniería en Sistemas<br>\n",
        "\n",
        "Ingeniería en Ciencias de la Computación<br>\n",
        "<br>\n",
        "RECUPERACION DE INFORMACIÓN - ICCD753<br>\n",
        "\n",
        "<br>\n",
        "Tema: Modelo Booleano de RI<br>\n",
        "Fecha de Entrega: 05-01-2022 <br>\n",
        "<br>\n",
        "Integrantes: <br>\n",
        "Luis Ernesto Almeida Zambrano<br>\n",
        "            Edison Daniel Cabrera Pabón"
      ],
      "metadata": {
        "id": "yG3sRjyVIEK-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejemplo Uno"
      ],
      "metadata": {
        "id": "3T1Bwa0EJWWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# implementation of vector space model for document retrieval\n",
        "\n",
        "import pandas\n",
        "# module to read the contents of the file from a csv file\n",
        "\n",
        "from contextlib import redirect_stdout\n",
        "# module to redirect the output to a text file\n",
        "\n",
        "import math\n",
        "# module to perform mathematical functions\n",
        "\n",
        "terms = []\n",
        "# list to store the terms present in the documents\n",
        "\n",
        "keys = []\n",
        "# list to store the names of the documents\n",
        "\n",
        "vec_Dic = {}\n",
        "# dictionary to store the name of the document and the weight as list\n",
        "\n",
        "dicti = {}\n",
        "# dictionary to store the name of the document and the terms present in it as a\n",
        "# vector\n",
        "\n",
        "dummy_List = []\n",
        "# list for performing some operations and clearing them\n",
        "\n",
        "term_Freq = {}\n",
        "# dictionary to store the term and the number of times of its occurrence in the\n",
        "# documents\n",
        "\n",
        "idf = {}\n",
        "# dictionary to store the term and the inverse document frequency\n",
        "\n",
        "weight = {}\n",
        "# dictionary to store the term and the weight which is the product of term\n",
        "# frequency and inverse document frequency\n",
        "\n",
        "\n",
        "def filter(documents, rows, cols):\n",
        "\t'''function to read and separate the name of the documents and the terms\n",
        "\tpresent in it to a separate list from the data frame and also create a\n",
        "\tdictionary which has the name of the document as key and the terms present\n",
        "\tin it as the list of strings which is the value of the key'''\n",
        "\n",
        "\tfor i in range(rows):\n",
        "\t\tfor j in range(cols):\n",
        "\t\t\t# traversal through the data frame\n",
        "\n",
        "\t\t\tif(j == 0):\n",
        "\t\t\t\t# first column has the name of the document in the csv file\n",
        "\t\t\t\tkeys.append(documents.loc[i].iat[j])\n",
        "\t\t\telse:\n",
        "\t\t\t\tdummy_List.append(documents.loc[i].iat[j])\n",
        "\t\t\t\t# dummy list to update the terms in the dictionary\n",
        "\n",
        "\t\t\t\tif documents.loc[i].iat[j] not in terms:\n",
        "\t\t\t\t\t# add the terms to the list if it is not present else continue\n",
        "\t\t\t\t\tterms.append(documents.loc[i].iat[j])\n",
        "\n",
        "\t\tcopy = dummy_List.copy()\n",
        "\t\t# copying the the dummy list to a different list\n",
        "\n",
        "\t\tdicti.update({documents.loc[i].iat[0]: copy})\n",
        "\t\t# adding the key value pair to a dictionary\n",
        "\n",
        "\t\tdummy_List.clear()\n",
        "\t\t# clearing the dummy list\n",
        "\n",
        "\n",
        "def compute_Weight(doc_Count, cols):\n",
        "\t'''Function to compute the weight for each of the terms in the document.\n",
        "\tHere the weight is calculated with the help of term frequency and\n",
        "\tinverse document frequency'''\n",
        "\n",
        "\tfor i in terms:\n",
        "\t\t# initially adding all the elements into the dictionary and initialising\n",
        "\t\t# the values as zero\n",
        "\t\tif i not in term_Freq:\n",
        "\t\t\tterm_Freq.update({i: 0})\n",
        "\n",
        "\tfor key, value in dicti.items():\n",
        "\t\t# to get the number of occurrence of each terms\n",
        "\t\tfor k in value:\n",
        "\t\t\tif k in term_Freq:\n",
        "\t\t\t\tterm_Freq[k] += 1\n",
        "\t\t\t\t# value incremented by one if the term is found in the documents\n",
        "\n",
        "\tidf = term_Freq.copy()\n",
        "\t# copying the term frequency dictionary to a dictionary named idf which is\n",
        "\t# further neede for computation\n",
        "\n",
        "\tfor i in term_Freq:\n",
        "\t\tterm_Freq[i] = term_Freq[i]/cols\n",
        "\t\t# term frequency is number of occurrence divided by total number of\n",
        "\t\t# documents\n",
        "\n",
        "\tfor i in idf:\n",
        "\t\tif idf[i] != doc_Count:\n",
        "\t\t\tidf[i] = math.log2(cols / idf[i])\n",
        "\t\t\t# inverse document frequency log of total number of documents divided\n",
        "\t\t\t# by number of occurrence of the terms\n",
        "\t\telse:\n",
        "\t\t\tidf[i] = 0\n",
        "\t\t\t# this is to avoid the zero division error\n",
        "\n",
        "\tfor i in idf:\n",
        "\t\tweight.update({i: idf[i]*term_Freq[i]})\n",
        "\t\t# weight is the product of term frequency and the inverse document\n",
        "\t\t# frequency\n",
        "\n",
        "\tfor i in dicti:\n",
        "\t\tfor j in dicti[i]:\n",
        "\t\t\tdummy_List.append(weight[j])\n",
        "\n",
        "\t\tcopy = dummy_List.copy()\n",
        "\t\tvec_Dic.update({i: copy})\n",
        "\t\tdummy_List.clear()\n",
        "\t\t# above operations performed to get the dictionary of weighted vector\n",
        "\t\t# for each of the documents\n",
        "\n",
        "\n",
        "def get_Weight_For_Query(query):\n",
        "\t'''function to get the weight for each terms present in the query, here we\n",
        "\tconsider the term frequency as the weight of the terms'''\n",
        "\n",
        "\tquery_Freq = {}\n",
        "\t# initialisation of the dictionary with query terms as key and its weight as\n",
        "\t# the values\n",
        "\n",
        "\tfor i in terms:\n",
        "\t\t# initially adding all the elements into the dictionary and initialising\n",
        "\t\t# the values as zero\n",
        "\t\tif i not in query_Freq:\n",
        "\t\t\tquery_Freq.update({i: 0})\n",
        "\n",
        "\tfor val in query:\n",
        "\t\t# to get the number of occurrence of each terms\n",
        "\t\tif val in query_Freq:\n",
        "\t\t\tquery_Freq[val] += 1\n",
        "\t\t\t# value incremented by one if the term is found in the documents\n",
        "\n",
        "\tfor i in query_Freq:\n",
        "\t\tquery_Freq[i] = query_Freq[i] / len(query)\n",
        "\t\t# term frequency obtained by dividing the number of occurrence of terms by\n",
        "\t\t# total number of terms in the query\n",
        "\n",
        "\treturn query_Freq\n",
        "\t# return the dictionary in which the key is the term and the value is the\n",
        "\t# weight\n",
        "\n",
        "\n",
        "def similarity_Computation(query_Weight):\n",
        "\t''' Function to calculate the similarity measure in which the weight of the\n",
        "\tquery and the document is multiplied in the numerator and the the weight is\n",
        "\tsquared and squareroot is taken the weights of the query and document'''\n",
        "\n",
        "\tnumerator = 0\n",
        "\tdenomi1 = 0\n",
        "\tdenomi2 = 0\n",
        "\t# initialisation of the variables with zero which is needed for computation\n",
        "\n",
        "\tsimilarity = {}\n",
        "\t# initialisation of dictionary which has the name of document as key and the\n",
        "\t# similarity measure as value\n",
        "\n",
        "\tfor document in dicti:\n",
        "\t\tfor terms in dicti[document]:\n",
        "\t\t\t# cosine similarity is calculated\n",
        "\n",
        "\t\t\tnumerator += weight[terms] * query_Weight[terms]\n",
        "\t\t\tdenomi1 += weight[terms] * weight[terms]\n",
        "\t\t\tdenomi2 += query_Weight[terms] * query_Weight[terms]\n",
        "\t\t\t# the summation values of the weight is calculated and later they are\n",
        "\t\t\t# divided\n",
        "\n",
        "\t\tif denomi1 != 0 and denomi2 != 0:\n",
        "\t\t\t# to avoid the zero division error\n",
        "\n",
        "\t\t\tsimi = numerator / (math.sqrt(denomi1) * math.sqrt(denomi2))\n",
        "\t\t\tsimilarity.update({document: simi})\n",
        "\t\t\t#dictionary is updated\n",
        "\n",
        "\t\t\tnumerator = 0\n",
        "\t\t\tdenomi2 = 0\n",
        "\t\t\tdenomi1 = 0\n",
        "\t\t\t# reinitialisation of the variables to zero\n",
        "\n",
        "\treturn (similarity)\n",
        "\t# the dictionary containing similarity measure as the value\n",
        "\n",
        "\n",
        "def prediction(similarity, doc_count):\n",
        "\t'''Function to predict the document which is relevant to the query '''\n",
        "\n",
        "\twith open('Salida.txt', 'w') as f:\n",
        "\t\twith redirect_stdout(f):\n",
        "\t\t\t# to redirect the output to a text file\n",
        "\n",
        "\t\t\tans = max(similarity, key=similarity.get)\n",
        "\t\t\tprint(ans, \"is the most relevant document\")\n",
        "\t\t\t# to print the name of the document which is most relevant\n",
        "\n",
        "\t\t\tprint(\"ranking of the documents\")\n",
        "\t\t\tfor i in range(doc_count):\n",
        "\t\t\t\tans = max(similarity, key=lambda x: similarity[x])\n",
        "\t\t\t\tprint(ans, \"rank is\", i+1)\n",
        "\t\t\t\t# to print the document name and its rank\n",
        "\n",
        "\t\t\t\tsimilarity.pop(ans)\n",
        "\n",
        "\n",
        "def main():\n",
        "\tdocuments = pandas.read_csv(r'DocumentoUno.csv')\n",
        "\t# to read the data from the csv file as a dataframe\n",
        "\n",
        "\trows = len(documents)\n",
        "\t# to get the number of rows\n",
        "\n",
        "\tcols = len(documents.columns)\n",
        "\t# to get the number of columns\n",
        "\n",
        "\tfilter(documents, rows, cols)\n",
        "\t# function call to read and separate the name of the documents and the terms\n",
        "\t# present in it to a separate list from the data frame and also create a\n",
        "\t# dictionary which has the name of the document as key and the terms present\n",
        "\t# in it as the list of strings which is the value of the key\n",
        "\n",
        "\tcompute_Weight(rows, cols)\n",
        "\t# Function to compute the weight for each of the terms in the document.\n",
        "\t# Here the weight is calculated with the help of term frequency and inverse\n",
        "\t# document frequency\n",
        "\n",
        "\tprint(\"Enter the query\")\n",
        "\tquery = input()\n",
        "\t# to get the query input from the user, the below input is given for obtaining\n",
        "\t# the output as in output.txt file\n",
        "\t# one three three\n",
        "\n",
        "\tquery = query.split(' ')\n",
        "\t# spliting the query as a list of strings\n",
        "\n",
        "\tquery_Weight = get_Weight_For_Query(query)\n",
        "\t# function call to get the weight for each terms present in the query, here we\n",
        "\t# consider the term frequency as the weight of the terms'''\n",
        "\n",
        "\tsimilarity = similarity_Computation(query_Weight)\n",
        "\t# Function call to calculate the similarity measure in which the weight of the\n",
        "\t# query and the document is multiplied in the numerator and the weight is\n",
        "\t# squared and squareroot is taken the weights of the query and document\n",
        "\n",
        "\tprediction(similarity, rows)\n",
        "\t# Function call to predict the document which is relevant to the query\n",
        "\n",
        "\n",
        "main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "N2nl8cf8IE-w",
        "outputId": "cb6bcf0d-01bf-4bb1-b2c7-9013d0d78894"
      },
      "execution_count": 15,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the query\n",
            "hockey\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-52e47e8a03a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-52e47e8a03a1>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;31m# squared and squareroot is taken the weights of the query and document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0mprediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m         \u001b[0;31m# Function call to predict the document which is relevant to the query\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-52e47e8a03a1>\u001b[0m in \u001b[0;36mprediction\u001b[0;34m(similarity, doc_count)\u001b[0m\n\u001b[1;32m    198\u001b[0m                         \u001b[0;31m# to redirect the output to a text file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m                         \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"is the most relevant document\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m                         \u001b[0;31m# to print the name of the document which is most relevant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejemplo Dos\n"
      ],
      "metadata": {
        "id": "b4gmyybCO-Qc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step-1 Importing the necessary libraries\n"
      ],
      "metadata": {
        "id": "-o7H1QTkPLom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from nltk.tokenize import sent_tokenize , word_tokenize\n",
        "import glob\n",
        "import re\n",
        "import os\n",
        "import numpy as np\n",
        "import sys\n",
        "Stopwords = set(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PS_5Ti9PO_v1",
        "outputId": "a54d7d16-218d-4269-aa80-74580c4ef7e3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-bc45972c3c87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mStopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step-2 Finding the set of unique words from all documents of the data set"
      ],
      "metadata": {
        "id": "T3dG0N84PM8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = []\n",
        "dict_global = {}\n",
        "file_folder = 'data/*'\n",
        "idx = 1\n",
        "files_with_index = {}\n",
        "for file in glob.glob(file_folder):\n",
        "    print(file)\n",
        "    fname = file\n",
        "    file = open(file , \"r\")\n",
        "    text = file.read()\n",
        "    text = remove_special_characters(text)\n",
        "    text = re.sub(re.compile('\\d'),'',text)\n",
        "    sentences = sent_tokenize(text)\n",
        "    words = word_tokenize(text)\n",
        "    words = [word for word in words if len(words)>1]\n",
        "    words = [word.lower() for word in words]\n",
        "    words = [word for word in words if word not in Stopwords]\n",
        "    dict_global.update(finding_all_unique_words_and_freq(words))\n",
        "    files_with_index[idx] = os.path.basename(fname)\n",
        "    idx = idx + 1\n",
        "    \n",
        "unique_words_all = set(dict_global.keys())"
      ],
      "metadata": {
        "id": "8xbWAlckPDVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step-3 Implementing helper functions"
      ],
      "metadata": {
        "id": "uDQDG_eQPQKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def finding_all_unique_words_and_freq(words):\n",
        "    words_unique = []\n",
        "    word_freq = {}\n",
        "    for word in words:\n",
        "        if word not in words_unique:\n",
        "            words_unique.append(word)\n",
        "    for word in words_unique:\n",
        "        word_freq[word] = words.count(word)\n",
        "    return word_freq\n",
        "def finding_freq_of_word_in_doc(word,words):\n",
        "    freq = words.count(word)\n",
        "        \n",
        "def remove_special_characters(text):\n",
        "    regex = re.compile('[^a-zA-Z0-9\\s]')\n",
        "    text_returned = re.sub(regex,'',text)\n",
        "    return text_returned"
      ],
      "metadata": {
        "id": "qzorogecPF0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step-4 Defining the linked list"
      ],
      "metadata": {
        "id": "1hH45vyMPTQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Node:\n",
        "    def __init__(self ,docId, freq = None):\n",
        "        self.freq = freq\n",
        "        self.doc = docId\n",
        "        self.nextval = None\n",
        "    \n",
        "class SlinkedList:\n",
        "    def __init__(self ,head = None):\n",
        "        self.head = head"
      ],
      "metadata": {
        "id": "DTkziMaNPVvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step-5 Making a linkedlist for each word and storing all the nodes (containing the file name and frequency of the respective word ) in the linkedlist."
      ],
      "metadata": {
        "id": "LdpnxAy2PbPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "linked_list_data = {}\n",
        "for word in unique_words_all:\n",
        "    linked_list_data[word] = SlinkedList()\n",
        "    linked_list_data[word].head = Node(1,Node)\n",
        "word_freq_in_doc = {}\n",
        "idx = 1\n",
        "for file in glob.glob(file_folder):\n",
        "    file = open(file, \"r\")\n",
        "    text = file.read()\n",
        "    text = remove_special_characters(text)\n",
        "    text = re.sub(re.compile('\\d'),'',text)\n",
        "    sentences = sent_tokenize(text)\n",
        "    words = word_tokenize(text)\n",
        "    words = [word for word in words if len(words)>1]\n",
        "    words = [word.lower() for word in words]\n",
        "    words = [word for word in words if word not in Stopwords]\n",
        "    word_freq_in_doc = finding_all_unique_words_and_freq(words)\n",
        "    for word in word_freq_in_doc.keys():\n",
        "        linked_list = linked_list_data[word].head\n",
        "        while linked_list.nextval is not None:\n",
        "            linked_list = linked_list.nextval\n",
        "        linked_list.nextval = Node(idx ,word_freq_in_doc[word])\n",
        "    idx = idx + 1"
      ],
      "metadata": {
        "id": "mvBjhejgPYp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step -6 Query processing and output generation"
      ],
      "metadata": {
        "id": "T_jMP6oHPfE8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = input('Enter your query:')\n",
        "query = word_tokenize(query)\n",
        "connecting_words = []\n",
        "cnt = 1\n",
        "different_words = []\n",
        "for word in query:\n",
        "    if word.lower() != \"and\" and word.lower() != \"or\" and word.lower() != \"not\":\n",
        "        different_words.append(word.lower())\n",
        "    else:\n",
        "        connecting_words.append(word.lower())\n",
        "print(connecting_words)\n",
        "total_files = len(files_with_index)\n",
        "zeroes_and_ones = []\n",
        "zeroes_and_ones_of_all_words = []\n",
        "for word in (different_words):\n",
        "    if word.lower() in unique_words_all:\n",
        "        zeroes_and_ones = [0] * total_files\n",
        "        linkedlist = linked_list_data[word].head\n",
        "        print(word)\n",
        "        while linkedlist.nextval is not None:\n",
        "            zeroes_and_ones[linkedlist.nextval.doc - 1] = 1\n",
        "            linkedlist = linkedlist.nextval\n",
        "        zeroes_and_ones_of_all_words.append(zeroes_and_ones)\n",
        "    else:\n",
        "        print(word,\" not found\")\n",
        "        sys.exit()\n",
        "print(zeroes_and_ones_of_all_words)\n",
        "for word in connecting_words:\n",
        "    word_list1 = zeroes_and_ones_of_all_words[0]\n",
        "    word_list2 = zeroes_and_ones_of_all_words[1]\n",
        "    if word == \"and\":\n",
        "        bitwise_op = [w1 & w2 for (w1,w2) in zip(word_list1,word_list2)]\n",
        "        zeroes_and_ones_of_all_words.remove(word_list1)\n",
        "        zeroes_and_ones_of_all_words.remove(word_list2)\n",
        "        zeroes_and_ones_of_all_words.insert(0, bitwise_op);\n",
        "    elif word == \"or\":\n",
        "        bitwise_op = [w1 | w2 for (w1,w2) in zip(word_list1,word_list2)]\n",
        "        zeroes_and_ones_of_all_words.remove(word_list1)\n",
        "        zeroes_and_ones_of_all_words.remove(word_list2)\n",
        "        zeroes_and_ones_of_all_words.insert(0, bitwise_op);\n",
        "    elif word == \"not\":\n",
        "        bitwise_op = [not w1 for w1 in word_list2]\n",
        "        bitwise_op = [int(b == True) for b in bitwise_op]\n",
        "        zeroes_and_ones_of_all_words.remove(word_list2)\n",
        "        zeroes_and_ones_of_all_words.remove(word_list1)\n",
        "        bitwise_op = [w1 & w2 for (w1,w2) in zip(word_list1,bitwise_op)]\n",
        "zeroes_and_ones_of_all_words.insert(0, bitwise_op);\n",
        "        \n",
        "files = []    \n",
        "print(zeroes_and_ones_of_all_words)\n",
        "lis = zeroes_and_ones_of_all_words[0]\n",
        "cnt = 1\n",
        "for index in lis:\n",
        "    if index == 1:\n",
        "        files.append(files_with_index[cnt])\n",
        "    cnt = cnt+1\n",
        "    \n",
        "print(files)"
      ],
      "metadata": {
        "id": "Ub4JclLhPg6W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}